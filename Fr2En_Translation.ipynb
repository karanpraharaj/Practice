{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fr2En-Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwIJGelkn1xO",
        "colab_type": "text"
      },
      "source": [
        "EN -> FR WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJzVYljSn-fG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pOFWAFUyJnI",
        "colab_type": "code",
        "outputId": "85c8dd43-f8ef-4c7d-dedf-ed8e114d0aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZXc1kFUCiYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngC_yGUqCkXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising language vocabulary\n",
        "class Lang: \n",
        "  def __init__(self,name):\n",
        "    self.name = name\n",
        "    self.word2ind = {}\n",
        "    self.word2count = {}\n",
        "    self.ind2word = {0:\"SOS\", 1:\"EOS\"}\n",
        "    self.n_words = 2  #counting SOS and EOS\n",
        "    \n",
        "  def addsentence(self,sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addword(word)\n",
        "      \n",
        "  def addword(self,word):\n",
        "    if word not in self.word2ind:\n",
        "      self.word2ind[word] = self.n_words\n",
        "      self.ind2word[self.n_words] = word    \n",
        "      self.n_words += 1    \n",
        "      self.word2count[word] = 1\n",
        "      \n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "      \n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnifUh9IGKjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427 \n",
        "\n",
        "def unicodeToAscii(s):       #Example : \"Kaká\" to \"Kaka\"\n",
        "    return ''.join( \n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'  \n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0saMlwxoLohJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the languages\n",
        "\n",
        "def readlangs(lang1,lang2,reverse=False):\n",
        "  print(\"Reading lines...\")\n",
        "  \n",
        "  #Read files and split into lines\n",
        "  lines = open('%s-%s.txt' %(lang1,lang2), encoding='utf-8')\n",
        "  lines = lines.read().strip().split('\\n')\n",
        "  \n",
        "  #Split every line into pairs and normalize\n",
        "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "  \n",
        "  # Reverse pairs, make Lang instances\n",
        "  if reverse:\n",
        "    pairs = [list(reversed(p)) for p in pairs]\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "  \n",
        "  else:\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    \n",
        "  return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7Z0gRA2Riea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "         len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "         p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E6P-a1hSIVm",
        "colab_type": "code",
        "outputId": "1277c450-3c8a-44cf-e3ad-f8ab9a0fe973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def preparedata(lang1,lang2, reverse = False):\n",
        "  input_lang, output_lang, pairs = readlangs(lang1,lang2,reverse)\n",
        "  print(\"Reading %s sentence pairs\" %len(pairs))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Trimmed to %s sentence pairs after filtering.\"%len(pairs))\n",
        "  \n",
        "  #Counting words in each language \n",
        "  print(\"Counting words...\")\n",
        "  for pair in pairs:\n",
        "    input_lang.addsentence(pair[0])\n",
        "    output_lang.addsentence(pair[1])\n",
        "  print(\"Counted words:\")\n",
        "  print(input_lang.name, input_lang.n_words)\n",
        "  print(output_lang.name, output_lang.n_words)\n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = preparedata('eng','fra', True)\n",
        "print(random.choice(pairs))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Reading 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs after filtering.\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['je crains qu il ne puisse venir .', 'i m afraid he cannot come .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHTtf6utg1bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size):\n",
        "    super(EncoderRNN,self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size)\n",
        "    \n",
        "  def forward(self,input,hidden):\n",
        "    embedded = self.embedding(input).view(1,1,-1)\n",
        "    output, hidden = self.gru(embedded,hidden)\n",
        "    return output,hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1, self.hidden_size, device = device)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvDJsjcaWvd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size):\n",
        "    super(DecoderRNN,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size,hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size)\n",
        "    self.out = nn.Linear(hidden_size,output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "  def forward(self,input,hidden):\n",
        "    output = self.embedding(input).view(1,1,-1)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output,hidden)\n",
        "    output = self.out(output[0])\n",
        "    output = self.softmax(output)\n",
        "    return output,hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f4WgrEEk-Ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size, dropoutp=0.1, max_length = MAX_LENGTH):\n",
        "    super(AttnDecoderRNN,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.dropoutp = dropoutp\n",
        "    self.max_length = max_length\n",
        "    \n",
        "    self.embedding = nn.Embedding(self.output_size,self.hidden_size)\n",
        "    self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
        "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.dropoutp = nn.Dropout(self.dropoutp) #probability of an element to be zeroed\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "    \n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input).view(1,1,-1)\n",
        "    embedded = self.dropoutp(embedded)\n",
        "    \n",
        "    attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))    \n",
        "    \n",
        "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "    output = self.attn_combine(output).unsqueeze(0)\n",
        "    \n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "\n",
        "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "    return output, hidden, attn_weights\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlgnc6EpFY-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the Training Data\n",
        "\n",
        "# To train, for each pair we will need an input tensor (indexes of the words in the input sentence) \n",
        "# and target tensor (indexes of the words in the target sentence). \n",
        "# While creating these vectors we will append the EOS token to both sequences.\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "  return [lang.word2ind[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "  indexes = indexesFromSentence(lang, sentence)\n",
        "  indexes.append(EOS_token)\n",
        "  return torch.tensor(indexes, dtype = torch.long, device = device).view(-1,1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "  return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQbYVmlO0nn",
        "colab_type": "text"
      },
      "source": [
        "Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VQQAfNmiYrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To train we run the input sentence through the encoder, \n",
        "# and keep track of every output and the latest hidden state. \n",
        "# Then the decoder is given the <SOS> token as its first input, \n",
        "# and the last hidden state of the encoder as its first hidden state.\n",
        "\n",
        "# TEACHER FORCING: \n",
        "# “Teacher forcing” is the concept of using the real target outputs as each next input, \n",
        "# instead of using the decoder’s guess as the next input. \n",
        "# Using teacher forcing causes it to converge faster but when the trained network is exploited, \n",
        "# it may exhibit instability.\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
        "          criterion, max_length = MAX_LENGTH):\n",
        "  \n",
        "  encoder_hidden = encoder.initHidden()\n",
        "  \n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  \n",
        "  input_length = input_tensor.size(0)\n",
        "  target_length = target_tensor.size(0)\n",
        "  \n",
        "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
        "  loss = 0\n",
        "  \n",
        "  for ei in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "  encoder_outputs[ei] = encoder_output[0,0]\n",
        "  \n",
        "  decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "  \n",
        "  decoder_hidden = encoder_hidden\n",
        "  \n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "  if use_teacher_forcing:\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "\n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "      decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "      decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    \n",
        "  else:\n",
        "    # Without teacher forcing: use its own predictions as the next input\n",
        "    \n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "      decoder_input, decoder_hidden, encoder_outputs)\n",
        "      \n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "      \n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "      \n",
        "      if decoder_input.item() == EOS_token :\n",
        "        break\n",
        "        \n",
        "  loss.backward()\n",
        "  \n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "   \n",
        "  return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keXH20GqdXuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Helper function to print elapsed time and remaining time\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQixu-LPuMqC",
        "colab_type": "text"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "- Start a timer\n",
        "- Initialize optimizers and criterion\n",
        "- Create set of training pairs\n",
        "- Start empty losses array for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWxh073ruVaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every = 1000, plot_every = 100, learning_rate = 0.01):\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  print_loss_total = 0\n",
        "  plot_loss_total = 0\n",
        "  \n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(),lr = learning_rate)\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(),lr = learning_rate)\n",
        "  \n",
        "  training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "  criterion = nn.NLLLoss()\n",
        "  \n",
        "  for iter in range(1, n_iters + 1):\n",
        "    training_pair = training_pairs[iter-1]\n",
        "    input_tensor = training_pair[0]\n",
        "    target_tensor = training_pair[1]\n",
        "    \n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    \n",
        "    print_loss_total += loss\n",
        "    plot_loss_total += loss\n",
        "    \n",
        "    if iter % print_every == 0:\n",
        "           print_loss_avg = print_loss_total / print_every\n",
        "           print_loss_total = 0\n",
        "           print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "           plot_loss_avg = plot_loss_total / plot_every\n",
        "           plot_losses.append(plot_loss_avg)\n",
        "           plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LdK0vHGwrVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNjnKguxFYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJYjsbXrxHBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Yz_YbAxKlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropoutp=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 25000, print_every=200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St9qqv1-xLKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTuLams7xTQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYNCVZsMxT26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}