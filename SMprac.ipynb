{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMprac.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzNKnmmiRZHR",
        "colab_type": "text"
      },
      "source": [
        "# **Sequence Models and LSTMs**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdG2b7oB_30x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6fa40a4-d632-4f71-966e-d919868072d0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) \n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fafc5165c70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g15Y19t0AJJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87176a5b-dcec-4428-f2fd-f88841f5fa61"
      },
      "source": [
        "lstm = nn.LSTM(3,3) #input dim =3 , op dim = 3\n",
        "inputs = [torch.randn(1,3) for _ in range(5)]  #Making a sequence length of 5\n",
        "\n",
        "# initialize the hidden state (2 states).\n",
        "hidden = (torch.randn(1, 1, 3),  #h_0\n",
        "          torch.randn(1, 1, 3))  #c_0\n",
        "\n",
        "#above line is same as \n",
        "# h0 = torch.randn(1, 1, 3)\n",
        "# c0 = torch.randn(1, 1, 3)\n",
        "\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    output, hidden = lstm(i.view(1,1,-1), hidden)\n",
        "\n",
        "print(output)\n",
        "print(hidden)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0628, -0.0462, -0.1530]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.0628, -0.0462, -0.1530]]], grad_fn=<StackBackward>), tensor([[[-0.4270, -0.0542, -0.4110]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc-1zrL6o1MA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "241f4b84-9817-4794-8657-b78279b522d1"
      },
      "source": [
        "torch.cat(inputs).view(len(inputs),1,-1)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5525,  0.6355, -0.3968]],\n",
              "\n",
              "        [[-0.6571, -1.6428,  0.9803]],\n",
              "\n",
              "        [[-0.0421, -0.8206,  0.3133]],\n",
              "\n",
              "        [[-1.1352,  0.3773, -0.2824]],\n",
              "\n",
              "        [[-2.5667, -1.4303,  0.5009]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnni5GKOyocJ",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm/48305882#48305882"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS10xO87EGdM",
        "colab_type": "text"
      },
      "source": [
        "# **LSTM for Part-of-Speech Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W1hDp1AcUVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aea9e3bb-b596-45f7-a003-343bc7a45b01"
      },
      "source": [
        "#Prepare Data\n",
        "\n",
        "def prepare_sequence(seq,to_ix):\n",
        "  idxs = [to_ix[w] for w in seq]\n",
        "  return torch.tensor(idxs, dtype = torch.long)\n",
        "\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\",\"NN\",\"V\",\"DET\",\"NN\"]),\n",
        "    (\"Everybody read that book.\".split(), [\"NN\",\"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "  for word in sent:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "\n",
        "tag_to_ix = {\"DET\" : 0, \"NN\" : 1, \"V\" : 2}\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book.': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQsYX6-2dOMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the model\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self,vocab_size, hidden_dim, embed_dim, tagset_size):\n",
        "    super(LSTMTagger,self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.word_embeddings = nn.Embedding(vocab_size,embed_dim)\n",
        "    \n",
        "    # The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hidden_dim.\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim)\n",
        "    \n",
        "    # The linear layer that maps from hidden state space to tag space\n",
        "    self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
        "    \n",
        "  def forward(self,sentence):\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence),1,-1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1))\n",
        "    tag_scores = F.log_softmax(tag_space,dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO6hNm0-uWw0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b2049183-5180-4165-fea2-71f25e039222"
      },
      "source": [
        "model = LSTMTagger(len(word_to_ix), HIDDEN_DIM, EMBEDDING_DIM, len(tag_to_ix))\n",
        "lossfn = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
        "\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    \n",
        "for epoch in range(300):\n",
        "  for sentence, tag in training_data:\n",
        "    \n",
        "    # Step 1. Remember that Pytorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "    # Tensors of word indices.\n",
        "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "    targets = prepare_sequence(tag, tag_to_ix)\n",
        "     \n",
        "    # Step 3 : Run forward pass\n",
        "    tag_preds = model(sentence_in)\n",
        "    \n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "    #  calling optimizer.step()\n",
        "    loss = lossfn(tag_preds,targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "  inputs = prepare_sequence(training_data[1][0], word_to_ix)\n",
        "  score = model(inputs)\n",
        "  print(score)\n",
        "\n",
        "    "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-5.3507, -0.0308, -3.6660],\n",
            "        [-2.9660, -3.4961, -0.0854],\n",
            "        [-0.0702, -5.2708, -2.7696],\n",
            "        [-5.3992, -0.0091, -5.3926]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}